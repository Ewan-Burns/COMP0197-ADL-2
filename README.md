# COMP0197: Weakly-supervised neural networks for segmentation

## Overview

Implements and evaluates a weakly-supervised segmentation framework based on Class Activation Maps (CAMs) and the Seed, Expand, Constrain (SEC) methodology. Also includes fully-supervised baseline models (U-Net and DeepLabV3) for comparison.

## Project Structure

-   `src/`:
    -   `MultiTargetOxfordPet.py`: Custom dataset class for handling different target types.
    -   `baseline/`: Implementations and training scripts for fully-supervised baselines (U-Net, DeepLabV3).
    -   `weakly_supervised/`: Implementation of the CAM+SEC weakly-supervised approach (ResNet, SEC loss, CAM generation).
    -   `utils/`: Utility functions (dataset transformations, Dice loss).
-   `data/`: Dataset (downnloaded).
-   `models/`: Saved model weights.

## Setup

1.  **Clone the repository:**
    ```bash
    git clone git@github.com:Ewan-Burns/COMP0197-ADL-2.git
    cd COMP0197-ADL-2
    ```
2.  **Activate the comp0197-cw1 Conda environment:**
    ```bash
    conda activate comp0197-cw1-pt
    ```
3.  **Install additional (development) dependencies:**
    ```bash
    pip install matplotlib tqdm
    ```

## Running the Code

All scripts should be run from the **root directory** of the project using the `python -m` flag, see the example below: 

```bash
python -m python -m src.baseline.train_unet
```

### 1. Train Fully-Supervised Baselines

These scripts train standard segmentation models using the full pixel-level segmentation masks provided in the Oxford-IIIT Pet dataset. They serve as a baseline for comparison with the later weakly-supervised approach. Both scripts train for a fixed num of epochs, save the trained model weights to the `./models/` directory and display some predictions post training.

**U-Net:**
Trains a U-Net model (common architecture for image segmentation), adapted here for the pet dataset. It uses a combination of Cross-Entropy Loss and Dice Loss for training.
```bash
python -m src.baseline.train_unet
```

**DeepLabV3:**
This script trains a DeepLabV3 model (architecture for semantic image segmentation) with a ResNet50 backbone (CNN acting as feature extractor). The final classification layer is adapted for the 3 classes (background, cat, dog). It also uses a combination of Cross-Entropy and Dice Loss.
```bash
python -m src.baseline.train_deeplabv3
```

### 2. Train Weakly-Supervised Model (ResNet + SEC)

This script implements the core weakly-supervised learning approach described in the project overview. It uses only image-level labels (presence of cat/dog) for supervision.
- Trains a `MultiHeadResNet` (`src/weakly_supervised/resnet.py`).
- Generates Class Activation Maps (CAMs) using the classification head.
- Trains the segmentation head using the SEC loss (`src/weakly_supervised/sec.py`), which incorporates:
    - Seed Loss: Matching CAM seeds.
    - Expand Loss: Using image-level labels to encourage class presence.
    - Constrain Loss: Refining predictions using DenseCRF based on image appearance.
The script trains for a fixed number of epochs and displays visualisations comparing the input image, the generated CAM, and the final predicted segmentation mask.
```bash
python -m src.weakly_supervised.train_resnet
```

### 3. Run GradCAM++ Visualisation Test

This script visualises CAMs generated by a standard ResNet18 using the `torchcam` library and applies CRF refinement.
```bash
python -m src.weakly_supervised.test_gradcamcpp
```
